{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NPS-23-005 \n",
    "\n",
    "Template: Getting_started.ipynb from hepdata_lib\n",
    "\n",
    "From hepdata_lib:\n",
    "The following instructions and examples should get you started to get your analysis into [HEPData](https://hepdata.net) using `hepdata_lib`. Please also refer to the [documentation](http://hepdata-lib.readthedocs.io/). While you can also run `hepdata_lib` on your local computer, you can use the [binder](https://mybinder.org/) or [SWAN](http://swan.cern.ch/) services in the browser. Mind that SWAN is only available for people with a CERN account.\n",
    "\n",
    "Also useful reference: https://github.com/jalimena/HepData_EXO-23-016/tree/main\n",
    "See \"main\" function in createHepData_all.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General setup\n",
    "\n",
    "To make sure things are working and `hepdata_lib` is available, run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hepdata_lib version 0.16.0\n"
     ]
    }
   ],
   "source": [
    "import hepdata_lib\n",
    "print(\"hepdata_lib version\", hepdata_lib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your HEPData submission\n",
    "\n",
    "The `Submission` object represents the whole HEPData entry and thus carries the top-level meta data that is equally valid for all the tables and variables you may want to enter. The object is also used to create the physical submission files you will upload to the HEPData web interface.\n",
    "\n",
    "When using `hepdata_lib` to make an entry, you always need to create a `Submission` object. Let's do that now, and then add data to it step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepdata_lib import Submission\n",
    "submission = Submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a `Submission` should contain details on the actual analysis such as it's abstract as well as links to the actual publication. The abstract should be in a plain text file. For `inspire` there's a special `record_id`, while for links to `arXiv` etc. one should use plain hyperlinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.read_abstract(\"NPS25003_inputs/abstract.txt\")\n",
    "\n",
    "#Production cross section\n",
    "\n",
    "#Pythia configurations\n",
    "\n",
    "#Signal model UFO Files\n",
    "\n",
    "#Generator Process cards\n",
    "\n",
    "#Cut flow tables\n",
    "\n",
    "#Data distributions of relevant ML input features\n",
    "\n",
    "#Small set of input vectors & ML outputs\n",
    "\n",
    "#Signal Efficiencies for simplified models' model points\n",
    "\n",
    "#Statistical model\n",
    "\n",
    "\n",
    "#submission.add_link(\"Webpage with all figures and tables\", \"https://cms-results.web.cern.ch/cms-results/public-results/publications/B2G-16-029/\")\n",
    "#submission.add_link(\"arXiv\", \"http://arxiv.org/abs/arXiv:1802.09407\")\n",
    "#submission.add_record_id(1657397, \"inspire\")\n",
    "#submission.add_additional_resource(\"Original abstract file\", \"example_inputs/abstract.txt\", copy_file=True)  # for illustration, probably not useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a table/figure\n",
    "\n",
    "In HEPData, figures and table will both be `Table` objects. The example here shows reading a plain text file containing the signal effiency times acceptance as a function of resonance mass for different signal models. The file has been uploaded to the `example_files` directory. For your submission, create a new directory, e.g. using the analysis identifier.\n",
    "\n",
    "Let's have a look at the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\t15\t47.6794\r\n",
      "50\t15\t33.4076\r\n",
      "60\t15\t35.9371\r\n",
      "70\t15\t21.6529\r\n",
      "80\t15\t22.4775\r\n",
      "90\t15\t29.3897\r\n",
      "100\t15\t38.8081\r\n",
      "110\t15\t28.113\r\n",
      "50\t20\t30.801\r\n",
      "60\t20\t14.8941\r\n"
     ]
    }
   ],
   "source": [
    "!head NPS25003_inputs/median_limits_allchannels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column is the mass value, the other columns contain the efficiency times acceptance values.\n",
    "\n",
    "Let's create the table/figure. First, we need to give it a name, which is usually just the identifier in the paper, here \"Figure 1\". The table also needs a description, which is usually the caption. You also need to describe the location, i.e. where to find it in the publication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepdata_lib import Table\n",
    "table = Table(\"Figure 7 Bottom Right\")\n",
    "table.description = \"Combined 95% CL observed upper limit on the cross section, using the BDT-based event categorization, as a function of scalar masses.\"\n",
    "table.location = \"Results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to provide more information on what is actually shown, which is done via `keywords`. The ones that are available can be taken from the documentation:\n",
    "- [Observables](https://hepdata-submission.readthedocs.io/en/latest/keywords/observables.html)\n",
    "- [Phrases](https://hepdata-submission.readthedocs.io/en/latest/keywords/phrases.html)\n",
    "- [Particles](https://hepdata-submission.readthedocs.io/en/latest/keywords/partlist.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#very unsure\n",
    "table.keywords[\"observables\"] = [\"SIG\"]\n",
    "table.keywords[\"phrases\"] = [\"Cross Section\"] #do I need phrases \n",
    "table.keywords[\"reactions\"] = [\"H --> {phi}_1 {phi}_2 --> 2{tau}4b/2{tau}2b\"]\n",
    "#do I need \"particles\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in the file. For this purpose, `numpy` is very handy. Since the first two rows are the header, we skip them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt(\"NPS25003_inputs/median_limits_allchannels.txt\", skiprows=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`numpy` stores the content as arrays. You can actually see that the entry that was labelled as `NaN` is correctly read in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 40.       15.       47.6794 ]\n",
      " [ 50.       15.       33.4076 ]\n",
      " [ 60.       15.       35.9371 ]\n",
      " [ 70.       15.       21.6529 ]\n",
      " [ 80.       15.       22.4775 ]\n",
      " [ 90.       15.       29.3897 ]\n",
      " [100.       15.       38.8081 ]\n",
      " [110.       15.       28.113  ]\n",
      " [ 50.       20.       30.801  ]\n",
      " [ 60.       20.       14.8941 ]\n",
      " [ 70.       20.       14.7805 ]\n",
      " [ 80.       20.       16.5984 ]\n",
      " [ 90.       20.       22.9752 ]\n",
      " [100.       20.       24.729  ]\n",
      " [ 70.       30.       33.5855 ]\n",
      " [ 80.       30.       29.888  ]\n",
      " [ 90.       30.       26.7847 ]\n",
      " [ 20.       15.        2.94763]\n",
      " [ 30.       15.        7.50414]\n",
      " [ 30.       20.        2.69308]\n",
      " [ 40.       20.        4.64869]\n",
      " [ 40.       30.        1.03961]\n",
      " [ 50.       30.        1.05894]\n",
      " [ 60.       30.        2.54844]\n",
      " [ 50.       40.        1.12181]\n",
      " [ 60.       40.        1.48343]\n",
      " [ 70.       40.        5.11465]\n",
      " [ 80.       40.        8.32959]\n",
      " [ 60.       50.        2.03326]\n",
      " [ 70.       50.        3.38112]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use this for our `Variable` definitions. The x-axis is usually the independent variable (`is_independent=True`), whereas the other ones are dependent (i.e. a function of the former). You also need to declare whether the variable is binned or not as well as the units. Similar as for the `keywords` used above, it is again important to provide additional information that can be found via the HEPData web interface using the observables and particles linked above. The values assigned are just slices of the `data` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hepdata_lib import Variable\n",
    "import numpy as np\n",
    "\n",
    "# Column meaning\n",
    "y_vals = data[:, 0]   # FIRST column = y bin centers\n",
    "x_vals = data[:, 1]   # SECOND column = x bin centers\n",
    "z_vals = data[:, 2]   # bin content\n",
    "\n",
    "# Build bin edges from centers\n",
    "def make_edges(centers):\n",
    "    centers = np.unique(centers.astype(float))\n",
    "    edges = np.zeros(len(centers) + 1)\n",
    "    edges[1:-1] = 0.5 * (centers[1:] + centers[:-1])\n",
    "    edges[0] = centers[0] - (edges[1] - centers[0])\n",
    "    edges[-1] = centers[-1] + (centers[-1] - edges[-2])\n",
    "    return centers, edges\n",
    "\n",
    "y_centers, y_edges = make_edges(y_vals)\n",
    "x_centers, x_edges = make_edges(x_vals)\n",
    "\n",
    "# Independent variables\n",
    "phi2_mass = Variable(\n",
    "    \"phi_2 mass\",\n",
    "    is_independent=True,\n",
    "    is_binned=True,\n",
    "    units=\"GeV\"\n",
    ")\n",
    "\n",
    "phi1_mass = Variable(\n",
    "    \"phi_1 mass\",\n",
    "    is_independent=True,\n",
    "    is_binned=True,\n",
    "    units=\"GeV\"\n",
    ")\n",
    "\n",
    "# Map center -> edge tuple\n",
    "y_edges_map = {y: (y_edges[i], y_edges[i+1]) for i, y in enumerate(y_centers)}\n",
    "x_edges_map = {x: (x_edges[i], x_edges[i+1]) for i, x in enumerate(x_centers)}\n",
    "\n",
    "# Only include bins that exist in your data\n",
    "phi2_mass.values = [y_edges_map[y] for y in y_vals]\n",
    "phi1_mass.values = [x_edges_map[x] for x in x_vals]\n",
    "\n",
    "# Dependent variable\n",
    "median_limit = Variable(\n",
    "    \"Median limit\",\n",
    "    is_independent=False,\n",
    "    is_binned=False,\n",
    "    units=\"pb\"\n",
    ")\n",
    "\n",
    "median_limit.values = [float(v) for y,x,v in data] \n",
    "# median_limit.add_qualifier(\"95% CL\", \"upper limit\")\n",
    "\n",
    "# Add to table\n",
    "table.add_variable(phi2_mass)\n",
    "table.add_variable(phi1_mass)\n",
    "table.add_variable(median_limit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of a plot, you should also add the original figure itself. `hepdata_lib` will take care of creating the thumbnail as well. Just add the figure as below.\n",
    "\n",
    "*WARNING*: This needs `ImageMagick` to be installed (this is the case when running on Binder and SWAN with LCG_94 or later). Executing the following line will fail if it is missing. In this case, comment out this line and restart from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table.add_image(\"NPS25003_inputs/plotLimit_2d_allchannels-2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, the original data file can be attached to the table as an additional resource file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.add_additional_resource(\"Original data file\", \"NPS25003_inputs/median_limits_allchannels.txt\", copy_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all that's needed for the table/figure. We still need to add it to the submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.add_table(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've added all tables/figures and the general submission details, you should add a few more keywords to all tables for better identification and searchability, e.g. the centre-of-mass energy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in submission.tables:\n",
    "    table.keywords[\"cmenergies\"] = [13000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create the submission for the upload. Here, we choose `example_output` as output directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent phi2_mass: [(35.0, 45.0), (45.0, 55.0), (55.0, 65.0), (65.0, 75.0), (75.0, 85.0)]\n",
      "Independent phi1_mass: [(12.5, 17.5), (12.5, 17.5), (12.5, 17.5), (12.5, 17.5), (12.5, 17.5)]\n",
      "Dependent median_limit (first 10): [47.6794, 33.4076, 35.9371, 21.6529, 22.4775, 29.3897, 38.8081, 28.113, 30.801, 14.8941]\n",
      "NaNs in dependent: 0\n",
      "Non-float dependent entries: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Independent phi2_mass:\", phi2_mass.values[:5])\n",
    "print(\"Independent phi1_mass:\", phi1_mass.values[:5])\n",
    "print(\"Dependent median_limit (first 10):\", median_limit.values[:10])\n",
    "print(\"NaNs in dependent:\", np.sum(np.isnan(median_limit.values)))\n",
    "print(\"Non-float dependent entries:\", sum(not isinstance(v,float) for v in median_limit.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"NPS25003_output\"\n",
    "submission.create_files(outdir, remove_old=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the working directory, you will now find a `submission.tar.gz` file, which you can use for uploading to your HEPData sandbox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.tar.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls submission.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `example_output` directory will contain the generated `yaml` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotLimit_2d_allchannels-2.png\tthumb_plotLimit_2d_allchannels-2.png\r\n",
      "submission.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!ls NPS25003_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\r\n",
      "additional_resources:\r\n",
      "- description: Created with hepdata_lib 0.16.0\r\n",
      "  location: https://doi.org/10.5281/zenodo.1217998\r\n",
      "comment: A search for Higgs boson decays to a pair of neutral scalars \\Paa and \\Pab\r\n",
      "  with unequal masses is performed in final states with \\PQb quarks and \\PGt leptons.\r\n",
      "  Depending on the masses of the neutral scalars, \\Pab can undergo a cascade decay\r\n",
      "  to a pair of \\Paa scalars. For both the cascade and non-cascade scenario, one of\r\n",
      "  the \\Paa is always considered to decay to \\PGt leptons, used for the online selection\r\n",
      "  of events. A data sample of proton-proton collisions at $\\sqrt{s}=13\\TeV$ corresponding\r\n",
      "  to an integrated luminosity of 138\\fbinv recorded with the CMS detector at the LHC\r\n",
      "  is analyzed. No statistically significant excess is observed over the standard model\r\n",
      "  backgrounds. Upper limits are set on the Higgs boson branching fraction to $\\Paa\\Pab\r\n",
      "  \\to 2\\PGt4\\PQb$ and to $\\Paa\\Pab \\to 2\\PGt2\\PQb$ along with the corresponding cross\r\n",
      "  sections. The limits on the cross section $\\sigma(\\PH \\to \\Paa \\Pab \\to 2\\PGt4\\PQb\r\n",
      "  / 2\\PGt2\\PQb)$ depend on the scalar masses and are observed to range between 0.8\r\n",
      "  and 37.8 pb. This analysis is sensitive to enhanced branching ratios of the neutral\r\n",
      "  scalars into \\PQb quarks and \\PGt leptons for some of the non-cascade mass points.\r\n",
      "data_license:\r\n",
      "  description: CC0 enables reusers to distribute, remix, adapt, and build upon the\r\n",
      "    material in any medium or format, with no conditions.\r\n",
      "  name: CC0\r\n",
      "  url: https://creativecommons.org/publicdomain/zero/1.0/\r\n"
     ]
    }
   ],
   "source": [
    "!cat NPS25003_output/submission.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: example_output/additional_figure_1.yaml: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cat NPS25003_output/figure_7_bottom_right.yaml"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
